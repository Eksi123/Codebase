{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入模块和文本数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import jieba\n",
    "from jieba import analyse\n",
    "from jieba import posseg\n",
    "import paddle\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "数据名：吉林省医保智慧医保智能客服平台热线工单数据\n",
    "数据量：91376\n",
    "采集时间：2022/06/01-2023/03/05\n",
    "采集指标：['工单属性', '工单编号', '状态', '工单来源', '当前处理人', '当期处理部门', '工单类型', '受理时间', '主叫电话',\n",
    "       '联系电话', '事项分类', '紧急程度', '处理期限', '完结时间', '处理时限', '诉求来源', '诉求人名称', '涉事标题',\n",
    "       '主体类型', '涉事主体', '诉求内容', '答复意见', '补充信息', '是否需要书面回复', '是否无效', '是否回访',\n",
    "       '是否保密', '附件数量']\n",
    "任务目标1：提取源数据中'涉事标题'和'诉求内容'文本数据，通过清洗，分词，去停用词，提取特征词等步骤初步了解医保事务中的热点话题\n",
    "任务目标2：知识图谱？贝叶斯网络？\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_excel(\"data/客服人员记录的工单-20220601-20230305.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本清洗、分词、去停用词、提取与医保事务相关的特征词\n",
    "\"\"\"\n",
    "涉事标题：短文本，内容部分规整，基本形式为“关于xxx的问题”或者直接为“xxx”，存在较多缺失值(29610)\n",
    "诉求内容：长文本，内容少数规整，多数填写较短，且存在部分无效填写或反映信息较少的情况，如“xxx接听”\n",
    "\n",
    "目标：就涉事标题和诉求内容分开进行处理，首先去除句中的多余空格和换行符，然后结合jieba中通用词典和自定义词典进行分词，同时做去停用词处理。\n",
    "将句子的分词结果按列表另存于txt文件中，最后再根据TextRank算法提取特征词\n",
    "\n",
    "特点：分词时采用了百度paddle（飞桨）深度学习模块训练序、列标注网络模型来实现分词，同时采用隐马尔可夫模型(HMM)识别词典中不存在的新词\n",
    "\"\"\"\n",
    "length = df.shape[0] # 文本条数\n",
    "\n",
    "# 1.文本清洗+分词+去停用词+保存（不含词性标注版本）\n",
    "f1 = open(\"data/涉事标题.txt\",'a+',encoding='utf-8') # 涉事标题\n",
    "f2 = open(\"data/诉求内容.txt\",'a+',encoding='utf-8') # 诉求内容\n",
    "f3 = open(\"data/stopwords.txt\",\"r\",encoding='utf-8') # 读取停用词表（原停用词表共1895词）\n",
    "stopwords = f3.read().split(\"\\n\")\n",
    "\n",
    "for i in range(length):\n",
    "    \"\"\"\n",
    "    涉事标题\n",
    "    \"\"\"\n",
    "    str1 = df['涉事标题'][i] # 读入句子\n",
    "    str1 = re.sub('\\s*|\\\\n',\"\",str1) # 剔除句中多余的空格，换行符等等\n",
    "    jieba.load_userdict('data/words.txt') # 除通用词典外，加载自定义词典\n",
    "    seg_list1 = jieba.cut(str1, cut_all=False, use_paddle=True, HMM=True) # 分词并返回迭代器（采用paddle精确分词模式，允许识别新词）\n",
    "    seg_list1 = (\"/\".join(seg_list1)).split('/') # 将分词结果存入列表\n",
    "    for j in range(len(seg_list1)): # 去停用词并将结果写入文档\n",
    "        if seg_list1[j] not in stopwords:\n",
    "            f1.write(str(seg_list1[j])+\" \") \n",
    "    f1.write(\"\\n\")\n",
    "    \"\"\"\n",
    "    诉求内容\n",
    "    \"\"\"\n",
    "    str2 = df['诉求内容'][i] \n",
    "    str2 = re.sub('\\s*|\\\\n',\"\",str2) \n",
    "    jieba.load_userdict('data/words.txt') \n",
    "    seg_list2 = jieba.cut(str2, cut_all=False, use_paddle=True, HMM=True) \n",
    "    seg_list2 = (\"/\".join(seg_list2)).split('/')\n",
    "    for k in range(len(seg_list2)): \n",
    "        if seg_list2[k] not in stopwords:\n",
    "            f2.write(str(seg_list2[k])+\" \") \n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "\n",
    "# 2.文本清洗+分词+去停用词+保存（含词性标注版本）\n",
    "from jieba import posseg\n",
    "f11 = open(\"data/涉事标题X.txt\",'a+',encoding='utf-8') \n",
    "f22 = open(\"data/诉求内容X.txt\",'a+',encoding='utf-8') \n",
    "f3 = open(\"data/stopwords.txt\",\"r\",encoding='utf-8') \n",
    "stopwords = f3.read().split(\"\\n\")\n",
    "\n",
    "for i in range(length):\n",
    "    \"\"\"\n",
    "    涉事标题\n",
    "    \"\"\"\n",
    "    str1 = df['涉事标题'][i] \n",
    "    str1 = re.sub('\\s*|\\\\n',\"\",str1) \n",
    "    jieba.load_userdict('data/words.txt') \n",
    "    seg_list1 = posseg.cut(str1, use_paddle=True, HMM=True) # 分词并返回迭代器（迭代器包含分词及对应词性）\n",
    "    seg_list1 = list(seg_list1) # 将分词结果转为列表\n",
    "    for j in range(len(seg_list1)): # 去停用词并将结果写入文档\n",
    "        if list(seg_list1[j])[0] not in stopwords:\n",
    "            f11.write(str(list(seg_list1[j]))+\" \") \n",
    "    f11.write(\"\\n\")\n",
    "    \"\"\"\n",
    "    诉求内容\n",
    "    \"\"\"\n",
    "    str2 = df['诉求内容'][i] \n",
    "    str2 = re.sub('\\s*|\\\\n',\"\",str2) \n",
    "    jieba.load_userdict('data/words.txt') \n",
    "    seg_list2 = posseg.cut(str2, use_paddle=True, HMM=True) \n",
    "    seg_list2 = list(seg_list2) # 将分词结果转为列表\n",
    "    for j in range(len(seg_list2)): # 去停用词并将结果写入文档\n",
    "        if list(seg_list2[j])[0] not in stopwords:\n",
    "            f22.write(str(list(seg_list2[j]))+\" \") \n",
    "    f22.write(\"\\n\")\n",
    "\n",
    "f11.close()\n",
    "f22.close()\n",
    "f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "[2023-09-05 15:52:10,701] [   DEBUG] __init__.py:113 - Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\XGQ\\AppData\\Local\\Temp\\jieba.cache\n",
      "[2023-09-05 15:52:10,703] [   DEBUG] __init__.py:132 - Loading model from cache C:\\Users\\XGQ\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.915 seconds.\n",
      "[2023-09-05 15:52:11,617] [   DEBUG] __init__.py:164 - Loading model cost 0.915 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "[2023-09-05 15:52:11,621] [   DEBUG] __init__.py:166 - Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF提取法\tTextRank提取法\n",
      "('异地', 0.8821972230099264) \t ('异地', 1.0)\n",
      "('就医', 0.6560433194917332) \t ('就医', 0.7056341064716433)\n",
      "('备案', 0.46587173931408965) \t ('备案', 0.5942878064930227)\n",
      "('参保', 0.35703466836509046) \t ('查询', 0.3636616178097616)\n",
      "('报销', 0.2762280062140543) \t ('报销', 0.35695631483500556)\n",
      "('查询', 0.21839318751230605) \t ('参保', 0.3433842718086795)\n",
      "('特病', 0.19857033705011312) \t ('办理', 0.24139158362621796)\n",
      "('医保卡', 0.17375554014855812) \t ('特病', 0.2007685574267731)\n",
      "('门诊', 0.11717505818696936) \t ('医保卡', 0.18455689056504054)\n",
      "('待遇', 0.1155907252896146) \t ('账户', 0.16592486668724693)\n",
      "('办理', 0.11256458201395744) \t ('单位', 0.15291740261060435)\n",
      "('基数', 0.10410686021071112) \t ('待遇', 0.15166533685864494)\n",
      "('特药', 0.09177489637169035) \t ('门诊', 0.15130419629551847)\n",
      "('在职', 0.09155591671309479) \t ('业务', 0.13976281807593946)\n",
      "('缴费', 0.09152681447178922) \t ('退休', 0.1358607345205681)\n",
      "('定点', 0.0877335803142852) \t ('定点', 0.12723211874520102)\n",
      "('账户', 0.08685785710979996) \t ('在职', 0.12616825980978655)\n",
      "('退休', 0.08175139146216301) \t ('新增', 0.11959009405857993)\n",
      "('催办', 0.08007324228567575) \t ('基数', 0.11789343710459133)\n",
      "('补缴', 0.0797849080054532) \t ('缴费', 0.1163010050453832)\n",
      "('共济', 0.07622230595133822) \t ('人员', 0.10882232265956662)\n",
      "('征缴', 0.0751514136992048) \t ('征缴', 0.09920202964484662)\n",
      "('额度', 0.0686372164560987) \t ('信息', 0.09827442536196986)\n",
      "('接续', 0.06842121376133464) \t ('结算', 0.0930334178376724)\n",
      "('来电', 0.06452281352393537) \t ('药店', 0.091290173173805)\n",
      "('余额', 0.06380128372942269) \t ('补缴', 0.0906184941644467)\n",
      "('业务', 0.062376369131856915) \t ('变更', 0.09038291391629534)\n",
      "('新增', 0.05760235033848826) \t ('余额', 0.09017117399136247)\n",
      "('报错', 0.056054920965100084) \t ('家庭', 0.08522837084323955)\n",
      "('退保', 0.05574998622105844) \t ('特药', 0.08521006928754356)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\pytorch\\lib\\site-packages\\jieba\\posseg\\__init__.py:16: DeprecationWarning: invalid escape sequence '\\.'\n",
      "  re_skip_detail = re.compile(\"([\\.0-9]+|[a-zA-Z0-9]+)\")\n",
      "d:\\miniconda\\envs\\pytorch\\lib\\site-packages\\jieba\\posseg\\__init__.py:17: DeprecationWarning: invalid escape sequence '\\.'\n",
      "  re_han_internal = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._]+)\")\n",
      "d:\\miniconda\\envs\\pytorch\\lib\\site-packages\\jieba\\posseg\\__init__.py:18: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  re_skip_internal = re.compile(\"(\\r\\n|\\s)\")\n",
      "d:\\miniconda\\envs\\pytorch\\lib\\site-packages\\jieba\\posseg\\__init__.py:21: DeprecationWarning: invalid escape sequence '\\.'\n",
      "  re_num = re.compile(\"[\\.0-9]+\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m text2 \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdata/诉求内容.txt\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m,encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mread()\n\u001b[0;32m     28\u001b[0m keywords21 \u001b[39m=\u001b[39m analyse\u001b[39m.\u001b[39mextract_tags(text2, topK\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, withWeight\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allowPOS\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mv\u001b[39m\u001b[39m'\u001b[39m)) \n\u001b[1;32m---> 29\u001b[0m keywords22 \u001b[39m=\u001b[39m analyse\u001b[39m.\u001b[39;49mtextrank(text2, topK\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, withWeight\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, allowPOS\u001b[39m=\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mv\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTF-IDF提取法\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTextRank提取法\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m30\u001b[39m):\n",
      "File \u001b[1;32md:\\miniconda\\envs\\pytorch\\lib\\site-packages\\jieba\\analyse\\textrank.py:84\u001b[0m, in \u001b[0;36mTextRank.textrank\u001b[1;34m(self, sentence, topK, withWeight, allowPOS, withFlag)\u001b[0m\n\u001b[0;32m     82\u001b[0m g \u001b[39m=\u001b[39m UndirectWeightedGraph()\n\u001b[0;32m     83\u001b[0m cm \u001b[39m=\u001b[39m defaultdict(\u001b[39mint\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mcut(sentence))\n\u001b[0;32m     85\u001b[0m \u001b[39mfor\u001b[39;00m i, wp \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words):\n\u001b[0;32m     86\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpairfilter(wp):\n",
      "File \u001b[1;32md:\\miniconda\\envs\\pytorch\\lib\\site-packages\\jieba\\posseg\\__init__.py:249\u001b[0m, in \u001b[0;36mPOSTokenizer.cut\u001b[1;34m(self, sentence, HMM)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcut\u001b[39m(\u001b[39mself\u001b[39m, sentence, HMM\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 249\u001b[0m     \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__cut_internal(sentence, HMM\u001b[39m=\u001b[39mHMM):\n\u001b[0;32m    250\u001b[0m         \u001b[39myield\u001b[39;00m w\n",
      "File \u001b[1;32md:\\miniconda\\envs\\pytorch\\lib\\site-packages\\jieba\\posseg\\__init__.py:226\u001b[0m, in \u001b[0;36mPOSTokenizer.__cut_internal\u001b[1;34m(self, sentence, HMM)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m blocks:\n\u001b[0;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m re_han_internal\u001b[39m.\u001b[39mmatch(blk):\n\u001b[1;32m--> 226\u001b[0m         \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m cut_blk(blk):\n\u001b[0;32m    227\u001b[0m             \u001b[39myield\u001b[39;00m word\n\u001b[0;32m    228\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 提取关键词并制作词云图\n",
    "\"\"\"\n",
    "jieba中analyse集成了分词，词性标注和关键词提取三个功能，使用十分方便。其中关键词提取分TF-IDF和TextRank两种提取思\n",
    "\n",
    "【1】TF-IDF\n",
    "其中TF为词频，指某个词在当前文档（已分词）的出现频率；IDF为逆文档频率，指包含该词的文档数占总文档数比例的倒数。\n",
    "可见TF和IDF越大，越有可能为关键词，在analyse模块中，已内置通用文档语料库，IDF值已经给定\n",
    "\n",
    "【2】TextRank\n",
    "TextRank算法是一种文本排序算法，由谷歌的网页重要性排序算法PageRank算法改进而来。分词完成后，TextRank算法基于相邻位置滑动窗口和频数计算，\n",
    "求得两个词之间的相似性。最后把词化作图的节点，相似性化作图的边，利用PageRank算法迭代计算每个词的rank,最后对rank排序以返回关键词\n",
    "\n",
    "在实际测试中，TF-IDF提取法的效果往往更好，一方面是利用了内置语料库中的IDF信息，另一方面TextRank算法对于待提取文本质量往往有更高要求\n",
    "此外，在进行分词与去停用词后，提取效果也会更好\n",
    "\n",
    "云图的制作借助了线上词云网站：易词云 https://www.yciyun.com/\n",
    "\"\"\"\n",
    "# 涉事标题\n",
    "text1 = open(\"data/涉事标题.txt\",\"r\",encoding='utf-8').read()\n",
    "keywords11 = analyse.extract_tags(text1, topK=30, withWeight=True, allowPOS=('n', 'v')) # TF-IDF（词频*逆文档频率）提取法\n",
    "keywords12 = analyse.textrank(text1, topK=30, withWeight=True, allowPOS=('n', 'v')) # TextRank提取法\n",
    "print('TF-IDF提取法'+\"\\t\"+'TextRank提取法')\n",
    "for i in range(30):\n",
    "    print(keywords11[i],'\\t',keywords12[i])\n",
    "\n",
    "# 诉求内容\n",
    "text2 = open(\"data/诉求内容.txt\",\"r\",encoding='utf-8').read()\n",
    "keywords21 = analyse.extract_tags(text2, topK=30, withWeight=True, allowPOS=('n', 'v')) \n",
    "keywords22 = analyse.textrank(text2, topK=30, withWeight=True, allowPOS=('n', 'v'))\n",
    "print('TF-IDF提取法'+\"\\t\"+'TextRank提取法')\n",
    "for i in range(30):\n",
    "    print(keywords21[i],'\\t',keywords22[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "涉事标题关键词聚类\n",
      "第1类关键词包含： 在职 基数 退休 \n",
      "\n",
      "第2类关键词包含： 办理 备案 定点 就医 异地 征缴 报销 报错 特病 特药 \n",
      "\n",
      "第3类关键词包含： 业务 余额 催办 待遇 接续 新增 查询 缴费 补缴 账户 门诊 额度 \n",
      "\n",
      "第4类关键词包含： 共济 \n",
      "\n",
      "第5类关键词包含： 医保卡 参保 来电 退保 \n",
      "\n",
      "诉求内容关键词聚类\n",
      "第1类关键词包含： 在职 基数 征缴 缴费 补缴 退休 \n",
      "\n",
      "第2类关键词包含： 业务 办理 医保卡 单位 参保 备案 定点 就医 异地 待遇 报销 报错 查询 特病 特药 经办人 账户 门诊 \n",
      "\n",
      "第3类关键词包含： 办结 告知 来电 解答 \n",
      "\n",
      "第4类关键词包含： 接听 \n",
      "\n",
      "第5类关键词包含： 催办 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 关键词聚类\n",
    "\"\"\"\n",
    "关键词表已由上述提取过程给出\n",
    "\n",
    "关键词聚类算法：\n",
    "1.相似性度量：假设每条文本工单代表一个完整的语义段（即事件表述完整），那么在同一条工单中两个词的同时出现某种意义上可以认为具有关联性，\n",
    "注：考虑到词向量矩阵稀疏的问题，故采取每10条工单词向量进行相加合并，即稀疏矩阵压缩\n",
    "\n",
    "2.聚类算法：余弦相似度+Kmeans聚类算法，聚类数目拟定为4或5\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "keywords_list1 = ['异地','就医','备案','参保','报销','查询','特病','医保卡','门诊','待遇','办理','基数','特药',\n",
    "'在职','缴费','定点','账户','退休','催办','补缴','共济','征缴','额度','接续','来电','余额','业务','新增','报错','退保']\n",
    "\n",
    "keywords_list2 = ['参保','异地','就医','备案','报销','查询','来电','办理','告知','办结','特病','医保卡','待遇','门诊',\n",
    "'经办人','接听','单位','缴费','基数','解答','定点','特药','账户','退休','在职','补缴','催办','征缴','报错','业务']\n",
    "\n",
    "length = df.shape[0]\n",
    "length_dense = length//10\n",
    "\n",
    "# 词向量矩阵构建与压缩\n",
    "# 涉事标题\n",
    "words1 = [] # 词库\n",
    "word_fea1 = None # 词向量特征\n",
    "wordvec1 = None # 词向量矩阵转置（仅包含关键词）\n",
    "wordvec1_dense = np.full((30,length_dense),0) # 压缩矩阵\n",
    "text1 = open(\"data/涉事标题.txt\",\"r\",encoding='utf-8') # 涉事标题分词文本\n",
    "for i in range(length):\n",
    "    temp = []\n",
    "    lines = text1.readline().split(\" \")[:-1]\n",
    "    for j in range(len(lines)):\n",
    "        if lines[j] in keywords_list1:\n",
    "            temp.append(lines[j])\n",
    "    words1.append(\" \".join(temp))\n",
    "model1 = CountVectorizer()\n",
    "bag1 = model1.fit_transform(words1) # bag是一个稀疏的矩阵。因为词袋模型就是一种稀疏的表示。\n",
    "word_fea1 = dict(zip(model1.vocabulary_.values(), model1.vocabulary_.keys())) # 输出单词与编号的映射关系\n",
    "wordvec1 = bag1.toarray().T # 调用稀疏矩阵的toarray方法，将稀疏矩阵转换为ndarray对象。\n",
    "for i in range(30):\n",
    "    for j in range(length_dense):\n",
    "        wordvec1_dense[i][j] = np.sum(wordvec1[i,j*10:j*10+10])\n",
    "\n",
    "Agg = AgglomerativeClustering(n_clusters=5, metric='cosine',linkage='complete').fit(wordvec1_dense)\n",
    "label1 = Agg.labels_\n",
    "\n",
    "print(\"涉事标题关键词聚类\")\n",
    "for i in range(5):\n",
    "    print(\"第\"+str(i+1)+'类关键词包含：',end=\" \")\n",
    "    for j in range(30):\n",
    "        if label1[j]==i:\n",
    "            print(word_fea1[j],end=\" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# 诉求内容\n",
    "words2 = [] \n",
    "word_fea2 = None\n",
    "wordvec2 = None \n",
    "text2 = open(\"data/诉求内容.txt\",\"r\",encoding='utf-8') \n",
    "wordvec2_dense = np.full((30,length_dense),0) \n",
    "for i in range(length):\n",
    "    temp = []\n",
    "    lines = text2.readline().split(\" \")[:-1]\n",
    "    for j in range(len(lines)):\n",
    "        if lines[j] in keywords_list2:\n",
    "            temp.append(lines[j])\n",
    "    words2.append(\" \".join(temp))\n",
    "model2 = CountVectorizer()\n",
    "bag2 = model2.fit_transform(words2)\n",
    "word_fea2 = dict(zip(model2.vocabulary_.values(), model2.vocabulary_.keys())) # 输出单词与编号的映射关系\n",
    "wordvec2 = bag2.toarray().T\n",
    "for i in range(30):\n",
    "    for j in range(length_dense):\n",
    "        wordvec2_dense[i][j] = np.sum(wordvec2[i,j*10:j*10+10])\n",
    "\n",
    "Agg = AgglomerativeClustering(n_clusters=5, metric='cosine',linkage='complete').fit(wordvec2_dense)\n",
    "label2 = Agg.labels_\n",
    "\n",
    "print(\"诉求内容关键词聚类\")\n",
    "for i in range(5):\n",
    "    print(\"第\"+str(i+1)+'类关键词包含：',end=\" \")\n",
    "    for j in range(30):\n",
    "        if label2[j]==i:\n",
    "            print(word_fea2[j],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33fd1f68ac8542c18b2f6e134ce0cc6203561395ef7997b293bef5a3412f400e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
