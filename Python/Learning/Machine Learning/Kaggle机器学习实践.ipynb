{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "泰坦尼克号乘客生还预测（Titanic - Machine Learning from Disaster）是Kaggle机器学习的新手任务，本文将以它为例来完整介绍机器学习的一般流程\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is my fist journey in Machine Learning!**\n",
    "\n",
    "By trandition, I will solve the classic \"Titanic survival\" problem by folowing four steps:\n",
    "\n",
    "1. Import modules and datasets needed\n",
    "2. Do an overview of datasets and a subsequent datasets preprocessing\n",
    "3. Build models and select the better performer\n",
    "4. Use the \"best model\" to do prediction \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Submission File Format:*\n",
    "\n",
    "*You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns \n",
    "(beyond PassengerId and Survived) or rows.*\n",
    "\n",
    "*The file should have exactly 2 columns:*\n",
    "\n",
    "*1. PassengerId (sorted in any order)*\n",
    "\n",
    "*2.Survived (contains your binary predictions: 1 for survived, 0 for deceased)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and datasets needed \n",
    "\n",
    "# Basical API\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint as sp_randint # give the integer random distribution \n",
    "\n",
    "# Sklearn support\n",
    "from sklearn import preprocessing # dataset preprocess\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif # feature selection\n",
    "from sklearn.model_selection import StratifiedKFold # k fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.linear_model import LogisticRegression # LR model\n",
    "from sklearn.svm import SVC # SVC model\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNC model\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # LDA model\n",
    "from sklearn.naive_bayes import GaussianNB # GNB model\n",
    "from sklearn.tree import DecisionTreeClassifier # DTC model\n",
    "from sklearn.ensemble import RandomForestClassifier # RFC model\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Import \"Titanic-surviviors\" datasets\n",
    "train_df = pd.read_csv('../input/titanic/train.csv')\n",
    "pred_df = pd.read_csv('../input/titanic/test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tips for new kagglers ###\n",
    "\n",
    "1. Before loading and reading the datasets, you must finish your phone verification, or you will get the error:\"No such file in directory\".\n",
    "\n",
    "2. The \"file (train.csv, test.csv, etc.) path\" can be copied ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an overview of datasets and a subsequent datasets preprocessing \n",
    "train_df.info()\n",
    "train_df.head()\n",
    "\n",
    "pred_df.info()\n",
    "pred_df.head()\n",
    "\n",
    "# Drop feature\n",
    "train_df = train_df.drop(labels = [\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\",\"Embarked\"], axis = 1)\n",
    "pred_df = pred_df.drop(labels = [\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\",\"Embarked\"], axis = 1)\n",
    "\n",
    "# Recode feature\n",
    "lab_encoder = preprocessing.LabelEncoder().fit([\"male\",\"female\"]) # \"female -> 0 & male -> 1\"\n",
    "train_df[\"Sex\"] = lab_encoder.transform(train_df[\"Sex\"])\n",
    "pred_df[\"Sex\"] = lab_encoder.transform(pred_df[\"Sex\"])\n",
    "\n",
    "# Fullfill feature\n",
    "combine_df = pd.concat([train_df.iloc[:,1:], pred_df.iloc[:,:]], axis = 0)\n",
    "imputer = KNNImputer(n_neighbors=3).fit(combine_df)\n",
    "train_df.iloc[:,1:] = imputer.transform(train_df.iloc[:,1:])\n",
    "pred_df.iloc[:,:] = imputer.transform(pred_df.iloc[:,:])\n",
    "\n",
    "# alter the type of features\n",
    "train_df[\"Survived\"] = train_df[\"Survived\"].astype(\"int\")\n",
    "train_df[\"Pclass\"] = train_df[\"Pclass\"].astype(\"int\")\n",
    "train_df[\"Sex\"] = train_df[\"Sex\"].astype(\"object\")\n",
    "train_df[\"Parch\"] = train_df[\"Parch\"].astype(\"object\")\n",
    "\n",
    "pred_df[\"Pclass\"] = pred_df[\"Pclass\"].astype(\"int\")\n",
    "pred_df[\"Sex\"] = pred_df[\"Sex\"].astype(\"object\")\n",
    "pred_df[\"Parch\"] = pred_df[\"Parch\"].astype(\"object\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview: ###\n",
    "\n",
    "1. The features: \"Name\",\"Sex\",\"Ticket\",\"Cabin\",\"Embarked\" is non-numerical.\n",
    "\n",
    "2. In train_df, the feature \"Cabin\" has a serious missing value problem (only 204 non-null), and \"Age\", \"Embarked\" have a slighter problem (714 and 899 non-null).\n",
    "\n",
    "3. In pred_df, the feature \"Cabin\" has a serious missing value problem (only 91 non-null), and \"Age\", \"Fare\" have a slighter problem (332 and 417 non-null)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Preprocess: ###\n",
    " \n",
    " 1. Drop the features: “PassengerId\",\"Name\",\"Ticket\",\"Cabin\",\"Embarked\",  because \"Name\",\"Ticket\",\"Embarked\" are almostly irrelatively with \"survive or not\", and the \"cabin\" has serious missing value problem in both two datasets.\n",
    " \n",
    " 2. Fullfill missing values of \"Age\" and the noly missing value of \"Fare\" in pred_df by linear regression method.\n",
    " \n",
    " 3. Recode the \"Sex\" (male, female) with number 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models and select the better performer \n",
    "Model = [LogisticRegression(), SVC(), KNeighborsClassifier(), LinearDiscriminantAnalysis(), GaussianNB(), DecisionTreeClassifier(),  RandomForestClassifier()]\n",
    "Res_acc = []; Res_f1 = []\n",
    "X_df = train_df.iloc[:,1:] # dataset of features (Pclass,Sex,...)\n",
    "Y_df = train_df.iloc[:,0] # dataset of label (Survived)\n",
    "\n",
    "for model in Model:\n",
    "    kfold = StratifiedKFold(n_splits = 10, random_state = 1, shuffle=True)\n",
    "    acc_results = cross_val_score(model, X_df, Y_df, cv = kfold, scoring = 'accuracy')\n",
    "    Res_acc.append(round(acc_results.mean(),2))\n",
    "    f1_results = cross_val_score(model, X_df, Y_df, cv = kfold, scoring = 'f1')\n",
    "    Res_f1.append(round(f1_results.mean(),2))\n",
    "\n",
    "print(\"accuracy for models:\"+\"\\n\"+  \n",
    "\"LogisticRegression: \"+str(Res_acc[0])+\"\\n\"+\n",
    "\"SVC: \"+str(Res_acc[1])+\"\\n\"+\n",
    "\"KNeighborsClassifier: \"+str(Res_acc[2])+\"\\n\"+\n",
    "\"LinearDiscriminantAnalysis: \"+str(Res_acc[3])+\"\\n\"+\n",
    "\"GaussianNB: \"+str(Res_acc[4])+\"\\n\"+\n",
    "\"DecisionTreeClassifier: \"+str(Res_acc[5])+\"\\n\"+\n",
    "\"RandomForestClassifier: \"+str(Res_acc[6])+\"\\n\"\n",
    ")\n",
    "\n",
    "print(\"f1-score for models:\"+\"\\n\"+  \n",
    "\"LogisticRegression: \"+str(Res_f1[0])+\"\\n\"+\n",
    "\"SVC: \"+str(Res_f1[1])+\"\\n\"+\n",
    "\"KNeighborsClassifier: \"+str(Res_f1[2])+\"\\n\"+\n",
    "\"LinearDiscriminantAnalysis: \"+str(Res_f1[3])+\"\\n\"+\n",
    "\"GaussianNB: \"+str(Res_f1[4])+\"\\n\"+\n",
    "\"DecisionTreeClassifier: \"+str(Res_f1[5])+\"\\n\"+\n",
    "\"RandomForestClassifier: \"+str(Res_f1[6])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best parameters of RandomForestClassifirer\n",
    "param_dist1 = {\"n_estimators\":sp_randint(1,51),\n",
    "              \"max_depth\": [3,4,5, None],                    \n",
    "              \"max_features\": sp_randint(0, 11),          \n",
    "              \"min_samples_split\": sp_randint(2, 11),    \n",
    "              \"bootstrap\": [True, False],                 \n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_dist1, n_iter=50, cv=10) # n_iter表示随机搜索20组，cv表示5折交叉验证\n",
    "random_search.fit(X_df, Y_df)\n",
    "print('best parameters:',random_search.best_params_,'\\n','best score:', random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the \"best model\" to do prediction\n",
    "RFC = RandomForestClassifier(\n",
    "                        bootstrap = True,\n",
    "                        criterion = 'gini',\n",
    "                        n_estimators = 20,\n",
    "                        max_depth = None,\n",
    "                        max_features = 2,\n",
    "                        min_samples_split = 10\n",
    ").fit(X_df, Y_df)\n",
    "pred_y = RFC.predict(pred_df)\n",
    "\n",
    "print(pred_y)\n",
    "\n",
    "df = pd.read_csv('../input/titanic/test.csv')\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": df[\"PassengerId\"],\n",
    "        \"Survived\": pred_y\n",
    "    })\n",
    "submission.to_csv('./submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
