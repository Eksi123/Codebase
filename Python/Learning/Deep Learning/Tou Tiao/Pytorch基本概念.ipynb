{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pytorch是Facebook研究团队所开发的一款深度学习框架，具备自主性强，易操作，模型稳定等优点，类似的框架还有TensorFlow\n",
    "\n",
    "下面是一些关于Pytorch进行深度学习的简单介绍和教程：\n",
    "1.https://www.zhihu.com/collection/710418581?page=2\n",
    "2.https://blog.csdn.net/sinat_39448069/article/details/120866541?spm=1001.2014.3001.5506\n",
    "3.https://blog.csdn.net/weixin_44216612/article/details/124203730?spm=1001.2014.3001.5506\n",
    "\n",
    "基于Pytorch进行深度学习主要分为以下几步：\n",
    "1. 封装并加载数据集\n",
    "2. 构建神经网络模型（网络框架、损失函数，损失优化策略）\n",
    "3. 模型的训练（训练函数，训练过程，有时候还需要调超参数）\n",
    "4. 保存模型\n",
    "5. 模型预测\n",
    "\n",
    "1.Dataset\n",
    "    1.1.batch_size\n",
    "    1.2.transform\n",
    "2.DataLoader\n",
    "3.Model\n",
    "    3.1.network\n",
    "    3.2.loss\n",
    "    3.3.optimizer\n",
    "        3.3.1.Learning rate\n",
    "        3.3.2.Early stopping\n",
    "4.分布式模型改造\n",
    "5.超参数优化\n",
    "6.checkpoint\n",
    "7.train 函数\n",
    "    7.1.Accuracy, recall 计算\n",
    "    7.2.Save model\n",
    "8.test 函数\n",
    "    8.1.记录 precision\n",
    "9.Matplotlib 绘图\n",
    "10.load model\n",
    "11.inference\n",
    "\n",
    "\n",
    "一个深度学习模型一般包含以下几个文件：\n",
    "datasets文件夹：存放需要训练和测试的数据集\n",
    "dataset.py：加载数据集，将数据集转换为固定的格式，返回图像集和标签集\n",
    "model.py：根据自己的需求搭建一个深度学习模型，具体搭建方法参考\n",
    "config.py：将需要配置的参数均放在这个文件中，比如batchsize，transform，epochs，lr等超参数\n",
    "train.py:加载数据集，训练\n",
    "predict.py：加载训练好的模型，对图像进行预测\n",
    "requirements.txt:一些需要的库，通过pip install -r requirements.txt可以进行安装\n",
    "readme：记录一些log\n",
    "log文件：存放训练好的模型\n",
    "loss文件夹：存放训练记录的loss图像\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.张量tensor\n",
    "\"\"\"\n",
    "张量涵盖了一般意义上的标量，向量，矩阵乃至更高维的数据，是进行深度学习的基础，通常与numpy搭配使用\n",
    "\n",
    "注：如非特地说明，本文中张量的阶和维概念一致\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 创建张量（还可使用Tensor函数创建默认为浮点型的张量）\n",
    "A0 = torch.tensor(1) # 0阶张量（标量）\n",
    "A1 = torch.tensor([1,2,3]) # 1阶张量（向量）\n",
    "A2 = torch.tensor([ # 2阶张量（矩阵）\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "])\n",
    "A3 = torch.tensor([ # 3阶张量\n",
    "    [[1,2,3],\n",
    "     [4,5,6],\n",
    "     [7,8,9]],\n",
    "\n",
    "     [[1.1, 2.2, 3.3],\n",
    "      [4.4, 5.5, 6.6],\n",
    "      [7.7, 8.8, 9.9]\n",
    "    ]])\n",
    "A3.type() # 数据类型，返回'torch.FloatTensor'\n",
    "A3.shape # 维度信息，返回torch.Size([2, 3, 3])\n",
    "\n",
    "\n",
    "# 特殊张量的创建\n",
    "A4 = torch.rand(3,3) # 0-1间的3x3均匀分布数据\n",
    "A5 = torch.randn(3,3) # 均值为0，方差为1的3x3正态分布数据\n",
    "\"\"\"\n",
    "还可使用numpy先创建列表，再转换为tensor，如下：\n",
    "A4 = np.random.uniform(size=9).reshape(3,3)\n",
    "A4 = torch.tensor(A4)\n",
    "A4 = numpy.array(A4) # 再次转换为numpy\n",
    "\n",
    "tensor比之numpy优势在于可以采用显卡加速运算\n",
    "\"\"\"\n",
    "A6 = torch.full([2,3],1) # 2阶（2x3）的值全为1的张量\n",
    "A7 = torch.arange(0,10,2) # 0到9的，步长为2的1阶张量\n",
    "A8 = torch.linspace(0,10,steps=4) # 1阶张量，将0-10平均切分为4份\n",
    "A9 = torch.eye(3) # 3x3单位矩阵\n",
    "\n",
    "# 张量基本运算\n",
    "# 切片运算\n",
    "A = torch.randn(4,4,4)\n",
    "A[1,[2,3],:3] # 获取第一阶上序号1，第二阶上序号2/3，以及第三阶上序号0/1/2的2x3数据\n",
    "\n",
    "# 排序\n",
    "A = torch.tensor([[1,4,3], [4,7,6], [10,9,7]])\n",
    "torch.sort(A,0,descending=False) # 按张量第一阶升序排序\n",
    "torch.topk(A,k=2,dim=0,largest=True) # 保留张量第一阶上前两个最大值\n",
    "\n",
    "# 算术运算\n",
    "A = torch.randn(4,4,4)\n",
    "A+3; A-3 # 元素+3/-3\n",
    "A.pow(2) # 平方\n",
    "A.pow(0.5) # 开方\n",
    "torch.exp(A) # 以e为底的指数\n",
    "torch.log(A) # 以e为底的对数\n",
    "torch.sum(A) # 所有元素求和，若为torch.sum(A,0)则表示按第一阶对元素求和\n",
    "torch.mean(A) # 所有元素求均值，若为torch.mean(A,0)则表示按第一阶对元素求均值（仅用于浮点型张量）\n",
    "torch.max(A) # 所有元素求最大值，其余同上\n",
    "torch.argmax(A,0) # 沿着张量第一阶返回最大值的索引\n",
    "torch.min(A) # 所有元素求最小值，其余同上\n",
    "torch.argmin(A,0) # 沿着张量第一阶返回最小值的索引\n",
    "\n",
    "\n",
    "# 广播运算\n",
    "\"\"\"\n",
    "两个张量进行广播运算的基本条件\n",
    "1.两张量阶数及对应维数相同\n",
    "2.两张量shape从后往前看，对应阶要么不存在，要么维数是1\n",
    "\"\"\"\n",
    "A = torch.randn(4,4,4)\n",
    "B = torch.randn(1,4)\n",
    "A+B\n",
    "A-B \n",
    "\n",
    "# 乘积运算\n",
    "A = torch.full((2,2,3),4)\n",
    "B = torch.full((2,2,3),3)\n",
    "C = torch.full((2,3,2),2)\n",
    "A*B # 阶数和对应长度相同的张量，对应位置元素相乘\n",
    "A@C # 张量相乘，最后两阶应遵循矩阵乘法规则\n",
    "\n",
    "# 张量变换：reshape、squeeze、flatten、cat、stack、transpose/permute\n",
    "# reshape重构张量的阶数和对应长度，与numpy中reshape功能一致，转换顺序由高阶到低阶，且转换前后元素个数不变\n",
    "A = torch.randn(2,3,4) # 三阶（2x3x4）\n",
    "A.reshape(6,4) # 二阶（6x4）\n",
    "A.reshape(2,12) # 二阶（2x12）\n",
    "A.reshape(1,2,3,4) # 四阶（1x2x3x4）\n",
    "A.reshape(-1,2,2,2) # 第一阶的长度会自动根据张量元素个数计算，结果为四阶（3x2x2x2)\n",
    "\n",
    "# squeeze压缩一个张量可以移除所有长度为1的轴axis，而unsqueeze解压一个张量则会在对应阶数增加一个长度\n",
    "A = torch.randn(1,12) # 二阶（1x12）\n",
    "A.squeeze() # 一阶（12）\n",
    "A.unsqueeze(dim=2) # 三阶（1x12x1）\n",
    "\n",
    "# flatten可以抹平张量对应阶以后所有阶数，将它完全展平，属于特殊的reshape函数\n",
    "A = torch.randn(2,3,4) # 三阶（2x3x4）\n",
    "A.flatten(start_dim=0) # 一阶（24）：将1阶及以上展平\n",
    "A.flatten(start_dim=1) # 二阶（2x12）：将2阶及以上展平\n",
    "\n",
    "# cat可以按对应阶进行张量拼接\n",
    "A = torch.randn(2,3)\n",
    "B = torch.rand(2,3) \n",
    "torch.cat((A,B),dim=1) # 二阶（2x6）：按一阶进行拼接，则二阶长度增加\n",
    "\n",
    "# stack则会增加一个新的阶，并在该阶上对两个张量进行堆叠\n",
    "A = torch.randn(2,3)\n",
    "B = torch.rand(2,3)\n",
    "torch.stack((A,B),dim=1) # 三阶（2x3x2）：按一阶进行堆叠\n",
    "\"\"\"\n",
    "tensor([[ 0.2775, -0.3184,  1.3068],\n",
    "        [-0.3685, -0.0498, -1.2364]]) \n",
    " tensor([[0.6206, 0.1927, 0.8001],\n",
    "        [0.9898, 0.4323, 0.9723]])\n",
    "tensor([[[ 0.2775,  0.6206],\n",
    "         [-0.3184,  0.1927],\n",
    "         [ 1.3068,  0.8001]], r\n",
    "\n",
    "        [[-0.3685,  0.9898],\n",
    "         [-0.0498,  0.4323],\n",
    "         [-1.2364,  0.9723]]])\n",
    "\"\"\"\n",
    "\n",
    "# transpose和permute可用于交换张量各阶，矩阵转置是其体现之一\n",
    "A = torch.randn(2,3,4)\n",
    "A.transpose(1,2) # 只能交换（输入）两个阶，交换后维度信息为（2，4，3）\n",
    "A.permute(1,2,0) # 能且必须交换（输入）所有阶，交换后维度信息为（3，4，2）\n",
    "# 类似的，numpy中transpose函数也可用于交换维度，如三维数组A，可用np.transpose(A,(1,2,0))交换维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Dataset、DataLoader封装和加载数据集\n",
    "\"\"\"\n",
    "知识点：\n",
    "Dataset： 覆写Dataset类用于对数据集进行封装，对数据进行预处理，清洗数据，记录 sample 与 label 的对应关系等等；\n",
    "DataLoader ：Dataloader类用于将 Dataset 封装成迭代器，将数据向量化，使之更适合加载进入神经网络。\n",
    "\"\"\"\n",
    "\n",
    "# 2.1.加载Pytorch的torchvision模块自带数据集，以FashionMNIST为例\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 训练集\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"Data\",  # 数据存储路径\n",
    "    train=True, # 下载训练集(True)或测试集(False)\n",
    "    download=True, # 是否从互联网下载Pytorch自带的数据（若数据已存在则不会重复下载）\n",
    "    transform=transforms.ToTensor() # 特征标签转换\n",
    ")\n",
    "\n",
    "# 测试集\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"Data\", \n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# 单一样本数据预览\n",
    "sample = next(iter(train_data)) # 将train_data转为迭代器，并取第一个数据\n",
    "\"\"\"\n",
    "其中iter函数可用于将列表、字典、字符串等转换为迭代器（可用循环整体调用，也可用next函数逐个调用）\n",
    "如 A = iter([1,2,3,4])\n",
    "1. for i in A: print(i) 输出为1 2 3 4\n",
    "2. next(A); next(A) 输出为1 2\n",
    "\"\"\"\n",
    "image,label = sample # image为图片数据，label为标签\n",
    "print(image.shape) # 输出图片数据格式：torch.Size([1, 28, 28])，其中颜色通道数为1，长宽各为28个数据位\n",
    "plt.imshow(image.squeeze(), cmap='gray') # 输出图片\n",
    "print('label:',label)\n",
    "\n",
    "# 批量样本数据预览\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=10) # 取10个样本\n",
    "batch = next(iter(train_loader))\n",
    "images,labels = batch\n",
    "print(images.shape) # 输出为torch.Size([10, 1, 28, 28])，其中样本数为10\n",
    "grid = torchvision.utils.make_grid(images,nrow=10) # 设置一个布局，将images中的图像按一行10个拼接输出\n",
    "plt.figure(figsize=(15,15)) # 设定画布大小\n",
    "plt.imshow(np.transpose(grid,(1,2,0))) # 调换图像各阶数据，将通道信息放在最后，便于显示\n",
    "print(labels)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "\n",
    "# 2.2.加载本地数据集，以图像数据hymenoptera_data为例\n",
    "\"\"\"\n",
    "从上面加载官方数据集不难看出，加载本地数据集我们同样需得到以下结果：\n",
    "【1】得到所有样本数据，样本数据标签及地址  \n",
    "【2】得到数据集长度 \n",
    "【3】能够得到指定位置或数量的数据集，以便后续的预处理操作 \n",
    "\n",
    "蚂蚁蜜蜂数据集下载链接: https://pan.baidu.com/s/1jZoTmoFzaTLWh4lKBHVbEA 密码: 5suq\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms,utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MyData(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, label_dir): # 初始化函数。提供数据地址和路径信息\n",
    "        self.root_dir = root_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.path = os.path.join(self.root_dir, self.label_dir)\n",
    "        self.img_path = os.listdir(self.path)\n",
    "    \n",
    "    def __len__(self): # 总样本数量\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, index): # 取出指定位置的数据内容和标签信息\n",
    "        img_item_name = self.img_path[index]\n",
    "        img_item_path = os.path.join(self.root_dir,  self.label_dir, img_item_name)\n",
    "        img = Image.open(img_item_path).convert('RGB')\n",
    "        transform = transforms.Compose([  # 图像变换\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.2,0.2,0.2], std=[0.5,0.5,0.5])\n",
    "                ])\n",
    "        img = transform(img)\n",
    "        return img, img_item_name\n",
    "\n",
    "root_dir = 'C:\\\\Users\\\\XGQ\\\\Desktop\\\\Programs\\\\Python\\\\Python程序\\\\Learning\\\\Deep Learning\\\\Data\\\\hymenoptera_data\\\\train'\n",
    "ants_label_dir = 'ants'\n",
    "ants_dataset = MyData(root_dir, ants_label_dir)\n",
    "\n",
    "# 单一样本数据预览\n",
    "image, label = ants_dataset.__getitem__(0)\n",
    "print('label:',label)\n",
    "image = np.transpose(image,(1,2,0))\n",
    "plt.imshow(image)\n",
    "\n",
    "# 批量样本数据预览\n",
    "loader = DataLoader(ants_dataset, batch_size=9, shuffle=True , num_workers=0, drop_last=False) # 将全部图片随机加载为9个一组的图片数据（不采取多线程加载，不丢弃剩余数据）\n",
    "batch  = next(iter(loader)) # 取第一组图片数据\n",
    "imgs,labels = batch # 取图片及标签\n",
    "\n",
    "grid = utils.make_grid(imgs,nrow=3) # 设置一个布局，将images中的图像按一行3个拼接输出\n",
    "plt.figure(figsize=(10,10)) # 设置画布大小\n",
    "plt.imshow(np.transpose(grid,(1,2,0))) # 调换图像各阶数据，将通道信息放在最后，便于显示\n",
    "for i in range(9): # 输出标签，3个为一行\n",
    "    if (i+1)%3!=0:\n",
    "        print(labels[i],end=\"\\t\")\n",
    "    else:\n",
    "        print(labels[i],end=\"\\n\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------#\n",
    "\n",
    "# 2.3.加载torchtext自带数据集，以文本数据为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.PIL与OpenCV\n",
    "\"\"\"\n",
    "PIL是Python的内置模块，对于一般的图像处理来说已经够用，更专业的则往往要用到OpenCV\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "\n",
    "# 3.1 PIL\n",
    "# 读入、展示并保存猫猫图片\n",
    "from PIL import Image\n",
    "img = Image.open('path')\n",
    "img.show()\n",
    "img.save('path',format=None) # 其中path为保存路径，format（可选）为代转换的图片格式\n",
    "\n",
    "# 输出类型、格式、尺寸、模式信息\n",
    "print(type(img), \"\\n\",img,format, \"\\n\",img.size, \"\\n\", img.mode)\n",
    "\n",
    "# 图片通道分离\n",
    "r,g,b = img.split() # 以三通道的jpg图像为例，可分割出R,G,B三通道的灰度图像\n",
    "\n",
    "# 获取图片像素数据矩阵\n",
    "matrix = np.array(img.getdata(band=None)) # 返回全部通道像素矩阵，其中band=0/1/2分别表示r,g,b单通道\n",
    "\n",
    "\n",
    "# 3.2 OpenCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.图像处理transform\n",
    "\"\"\"\n",
    "transform是深度学习torchvision模块下用于图像处理的专用模块，本文介绍它常用的几个函数并作演示\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# 打开图片\n",
    "image = Image.open('C:\\\\Users\\\\XGQ\\\\Desktop\\\\Programs\\\\Python\\\\Python程序\\\\Learning\\\\Deep Learning\\\\Data\\\\pokeman\\\\pikachu\\\\00000000.jpg')\n",
    "image.show()\n",
    "\n",
    "# 提取出图片各通道的像素数据，并转换为张量格式\n",
    "toTensor = transforms.ToTensor()\n",
    "tensor_img = toTensor(image)\n",
    "print(tensor_img)\n",
    "\n",
    "# 将张量数据转为图片\n",
    "toPILImage = transforms.ToPILImage()\n",
    "img = toPILImage(tensor_img)\n",
    "img.show()\n",
    "\n",
    "# 调整图片大小（原理为图片的上下采样）\n",
    "resize = transforms.Resize((600,500)) # 调整后高=600, 宽=500\n",
    "resize_img = resize(img)\n",
    "resize_img.show()\n",
    "\n",
    "# 图片标准化\n",
    "normalize = transforms.Normalize(mean=[0.2,0.2,0.2], std=[0.5,0.5,0.5])\n",
    "norm_img = toPILImage(normalize(tensor_img))\n",
    "norm_img.show()\n",
    "\n",
    "# Compose整合各对象\n",
    "image = Image.open('C:\\\\Users\\\\XGQ\\\\Desktop\\\\Programs\\\\Python\\\\Python程序\\\\Learning\\\\Deep Learning\\\\Data\\\\pokeman\\\\pikachu\\\\00000000.jpg')\n",
    "transform = transforms.Compose([\n",
    "    transforms.resize((600,500)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.2,0.2,0.2], std=[0.5,0.5,0.5])\n",
    "    ])\n",
    "img = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.可视化工具tensorboard.SummaryWriter\n",
    "\"\"\"\n",
    "tensorboard原是TensorFlow的可视化工具，因方便好用故被Pytorch学习，从1.2.0版本开始支持tensorboard，前版本可用tensorboardX代替\n",
    "\n",
    "tensorboard的工作原理：把深度学习项目中所关心的数据（包括图片，文字，图表等等）存入到某文件夹中，再取出该文件夹的数据展示于浏览器\n",
    "需注意数据只能为tensor、np.array或string类型\n",
    "\n",
    "展示方法：vscode有内置的tensorboard插件，也可在对应环境的终端输入tensorboard --logdir=\"文件夹名\"\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from torchvision import datasets,transforms\n",
    "\n",
    "# 下载并加载CTFAR10测试数据集\n",
    "dataset = datasets.CIFAR10(root='../Deep Learning/Data', train=False, download=True, transform=transforms.ToTensor()) \n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "# 设定SummaryWriter文件夹目录\n",
    "writer = SummaryWriter('../Deep Learning/logs/log1')\n",
    "\n",
    "# add_images函数\n",
    "step = 0\n",
    "for data in dataloader:\n",
    "    imgs, labels = data\n",
    "    \"\"\"\n",
    "    tag: 图像数据名\n",
    "    img_tensor: 图像数据\n",
    "    global_step: 全局步长值\n",
    "    dataformats: 匹配图像数据格式（CHW为通道数+高+宽（默认）、HWC为高+宽+通道数、HW为高+宽）\n",
    "    \"\"\"\n",
    "    writer.add_images(tag=\"images\", img_tensor=imgs, global_step=step) # dataformats默认为size数+CHW\n",
    "    step += 1\n",
    "writer.close()\n",
    "\n",
    "# add_scalar函数\n",
    "for step in range(20):\n",
    "    writer.add_scalar(tag=\"graph\",scalar_value=step*3, global_step=step) # 单曲线绘图，scalar_value相当于图像Y值，global_step相当于X值\n",
    "    writer.add_scalars(tag=\"graphs\",tag_scalar_dict={'graph1':step*2, 'graph2':step*0.5}, global_step=step) # 多曲线绘图，tag_scalar_dict相当于图像Y值\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.模型的查看，保存与加载\n",
    "\"\"\"\n",
    "深度学习模型的保存，通常是指模型训练过程中或训练结束后，将学习好的模型参数（不建议保存模型本身）保存起来留作调用。\n",
    "深度学习模型的加载，是指在模型训练开始前，或者使用模型做预测时，将保存的模型参数加载到网络中\n",
    "深度学习模型查看主要指网络结构和参数的查看\n",
    "\"\"\"\n",
    "model = \"cnn\"\n",
    "optimizer =  torch.optim.SGD(model.parameters())\n",
    "epoch = 100\n",
    "loss = None\n",
    "\n",
    "# 保存加载方式1(model.state_dict字典)\n",
    "torch.save(model.state_dict(), '保存路径/model_state_dict.pth') # 网络参数保存\n",
    "model.load_state_dict(torch.load('保存路径/model_state_dict.pth')) # 网络参数加载\n",
    "\n",
    "# 保存加载方式2(checkpoint字典)\n",
    "# 保存checkpoint（方式1的plus版本，还可保存优化器参数，损失等信息）\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict':model.state_dict(),\n",
    "            'optimizer_state_dict':optimizer.state_dict(),\n",
    "            'loss':loss}, \n",
    "            '保存路径/model_checkpoint.tar')   # 这里的后缀名官方推荐使用.tar            \n",
    "\n",
    "# 加载checkpoint\n",
    "checkpoint = torch.load('保存路径/model_checkpoint.tar')    # 加载checkpoint\n",
    "model.load_state_dict(checkpoint['model_state_dict']) # 网络参数加载\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict']) # 优化器参数加载\n",
    "epoch = checkpoint['epoch'] # epoch加载\n",
    "loss = checkpoint['loss'] # loss加载\n",
    "\n",
    "# 模型的查看\n",
    "for name, net in model.named_modules(): # 查看网络结构\n",
    "    print(name,\":\",net)\n",
    "for name, param in model.named_parameters(): # 查看网络参数，name为参数名\n",
    "    print(name,\":\",param.shape) \n",
    "# 除此以外，还可对保存的参数如model.state_dict和optimizer.state_dict等进行查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.模型参数的初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  71.3695,  272.2614, -188.8705,  ...,   61.2419, -212.0480,\n",
       "         -108.5798],\n",
       "        [ -21.6821,  192.0014,  107.6483,  ...,  141.7072,   84.5083,\n",
       "           65.1910],\n",
       "        [-186.6979,  -25.2006,   29.9123,  ..., -282.3174,  -70.1695,\n",
       "           40.0141],\n",
       "        ...,\n",
       "        [   6.0081,   92.3619, -286.1390,  ...,  152.2972,   -8.6563,\n",
       "          -44.9335],\n",
       "        [ 143.2192,   42.6328,  -52.0900,  ...,   16.5236,  -79.0288,\n",
       "           58.1762],\n",
       "        [ -49.7891,   85.4641,  -35.1401,  ...,  -64.5795,  -20.2004,\n",
       "           16.2153]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.利用GPU来加速计算（没太搞懂）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
